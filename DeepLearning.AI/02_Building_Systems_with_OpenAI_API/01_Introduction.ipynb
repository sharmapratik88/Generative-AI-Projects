{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLOLqWMZju+XQ2XoJ4CwXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmapratik88/LearningAI/blob/main/DeepLearning.AI/02_Building_Systems_with_OpenAI_API/01_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Systems with the ChatGPT API\n",
        "A [DeepLearning.AI](https://www.deeplearning.ai/short-courses/) short course taught by Andrew NG and Isa Fulford (OpenAI).\n",
        "\n",
        "Coverage:\n",
        "- Building a system that requires more than a single prompt or a single call to an LLM or a large language model\n",
        "- How do you evaluate the input to ensure it doesn't contain problematic content, such as hateful speech, or isn't providing inaccurate or inappropriate answers?\n",
        "- How do we sequentially process the user input in multiple steps to get to the final output shown to the user?\n",
        "- Split complex tasks into a pipeline of subtasks using multistage prompts."
      ],
      "metadata": {
        "id": "3D-fQqiUhCIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNt-EOk7RC2L",
        "outputId": "80b01dc4-072b-4468-9a8b-07f85eaf9a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the current working directory\n",
        "import os; os.chdir('/content/drive/MyDrive/Learnings/DeepLearning.AI/Building_Systems_with_ChatGPT_API')"
      ],
      "metadata": {
        "id": "6B2xaPphRJCK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages/libraries\n",
        "!pip install -q openai\n",
        "\n",
        "from helperFunc import *"
      ],
      "metadata": {
        "id": "oYowacq2TTzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4374ce-4e48-426e-8e86-8c4d53332849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/225.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Model (LLMs)\n",
        "\n",
        "LLMs are sophisticated computer programs capable of generating and analyzing human-quality text and have revolutionized various aspects of Natural Language Processing (NLP).\n",
        "\n",
        "Read more at [ChatGPT Prompt Engineering & LLMs](https://pratikdsharma.com/chatgpt-prompt-engineering/).\n",
        "\n",
        "### Answering how do they work?\n",
        "- For e.g., text generation, prompt: \"I love eating,\" and LLM generates the rest, which might be \"bagels with cream cheese or out with friends.\"\n",
        "\n",
        "- The main tool to train an LLM is supervised learning (x -> y), which predicts the next word repeatedly. For example, your training sets contain a lot of text data, which says, \"My favorite food is a bagel with cream cheese and lox.\" Then, when asking LLM the above prompt again, it might respond to the next word as \"bagel\".\n",
        "\n",
        "- Two major LLMs:\n",
        "  - base LLMs: predicts next word, based on text training data\n",
        "  - instruction LLMs: Try to follow instructions\n",
        "  - Read more at [ChatGPT Prompt Engineering & LLMs](https://pratikdsharma.com/chatgpt-prompt-engineering/).\n",
        "\n",
        "  **How do we go from base to instruction-tuned LLMs?**\n",
        "  - Train a base LLM on loads of data\n",
        "  - Further, train the model:\n",
        "\t- Fine-tune on examples of where the output follows input instructions.\n",
        "   - Obtain human ratings of the quality of different LLM outputs based on whether it is helpful, honest, or harmless.\n",
        "\t- Tune LLM to increase the probability that it generates more highly rated outputs (using RHLF: Reinforcement Learning from Human Feedback).\n",
        "\t- Base LLM training can take months. For e.g., the process of going from the Base LLM to the Instruction Tuned LLM can be done in maybe days on a much more modest size dataset and much more modest size computational resources."
      ],
      "metadata": {
        "id": "DVAm3SRJbATX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion('What is the capital of France?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYLqg0RRaQSB",
        "outputId": "45d3aa7d-7511-4140-b6fa-658e0e4998aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion('Take the letters in lollipop and reverse them!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_Jy2aKh9CN",
        "outputId": "31cd9ad9-4d0f-4c9d-a682-718b950e1d6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reversed letters of \"lollipop\" are \"pillipol\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importance of Tokens\n",
        "  - LLM will take sequences of characters, like \"Learning new things is fun!\" and group the characters together to form tokens that comprise commonly occurring sequences of characters. Since each is a fairly common word, each token in the earlier example will correspond to one word or word in a space.\n",
        "  - In example such as \"Prompting as powerful developer tool\", it will be actually broken down to three tokens with \"prom\", \"pt\", and \"ing\" because those three are commonly occuring sequences of letters.\n",
        "  - For word lollipop, the tokenizer will break this down into three tokens, \"l\", \"oll\", and \"ipop\"."
      ],
      "metadata": {
        "id": "Mh4uQwnZiRPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion('Take the letters in l-o-l-l-i-p-o-p and reverse them!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8ninnCYiMzW",
        "outputId": "cb5b4f08-c922-446d-ed66-369c11c93f2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-o-p-i-l-l-o-l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, each letter was accompanied by dashes in between the letters, LLM tokenizes each of these characters into an individual toekn, making it easier for it to see the individual letters and print them out in reverse order.\n",
        "\n",
        "For English language input, 1 token is around 4 characters or 3/4 of a word.\n",
        "\n",
        "**Token limits:**\n",
        "- Different models have different limits on the number of token in input (aka context) + output (aka completion)\n",
        "- For e.g. for gpt3.5-turbo is ~4000 tokens (input + output)\n",
        "- For an input context that is much longer than this, it will either throw an exception or generate an error.\n",
        "- The limit for a given API request is the combination of the prompt and the completion. For example, if the prompt is 3,000 tokens, the completion cannot be more than around 1,000 tokens.\n"
      ],
      "metadata": {
        "id": "Btkl1G94l-ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Message roles:**\n",
        "- System: sets tone/behavior of assistant\n",
        "- user: prompts\n",
        "- assistant: LLM response"
      ],
      "metadata": {
        "id": "ku1PQTparTE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "    {'role':'system',\n",
        "     'content':\"\"\"You are an assistant who\n",
        "                  responds in the style of Dr Seuss.\n",
        "                \"\"\"\n",
        "                  },\n",
        "    {'role':'user',\n",
        "     'content':\"\"\"write me a very short poem\n",
        "                    about a happy carrot.\n",
        "                \"\"\"\n",
        "                    },\n",
        "]\n",
        "\n",
        "res = get_completion_from_messages(messages, temp=1)\n",
        "print(f'Response: {res}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKfSs_xFpBfq",
        "outputId": "00dcb41b-36e5-4d81-e589-7ecd69e95505"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In a garden so bright and cheery,\n",
            "Lived a carrot, oh so merry!\n",
            "With a vibrant orange hue,\n",
            "And a smile that just grew and grew.\n",
            "\n",
            "With each sunny day that passed,\n",
            "This happy carrot danced and laughed.\n",
            "Roots deeply planted in the ground,\n",
            "Its joyous nature could easily be found.\n",
            "\n",
            "From leafy greens to tender core,\n",
            "This carrot radiated joy and more.\n",
            "A crunchy delight in every bite,\n",
            "Bringing happiness day and night.\n",
            "\n",
            "So remember, dear friend, from this brief tale,\n",
            "To be like the carrot, never fail.\n",
            "Choose to bring happiness and cheer,\n",
            "Spread joy to all, far and near!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting response length as a system message!\n",
        "messages =  [\n",
        "    {'role':'system',\n",
        "     'content':\"\"\"You are an assistant who\n",
        "                  responds in the style of Dr Seuss.\n",
        "                  Your response must be one sentence long.\n",
        "                \"\"\"\n",
        "                  },\n",
        "    {'role':'user',\n",
        "     'content':\"\"\"write me a very short poem\n",
        "                    about a happy carrot\n",
        "                \"\"\"\n",
        "                    },\n",
        "]\n",
        "\n",
        "res = get_completion_from_messages(messages, temp=1)\n",
        "print(f'Response: {res}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGofwYaQp_Vn",
        "outputId": "eab28422-18cd-4dda-f3bf-104eae3d04cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In the garden, a carrot so orange and bright, filled with joy and delight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the token counts!\n",
        "messages =  [\n",
        "    {'role':'system',\n",
        "     'content':\"\"\"You are an assistant who\n",
        "                  responds in the style of Dr Seuss.\n",
        "                  Your response must be one sentence long.\n",
        "                \"\"\"\n",
        "                  },\n",
        "    {'role':'user',\n",
        "     'content':\"\"\"write me a very short poem\n",
        "                    about a happy carrot\n",
        "                \"\"\"\n",
        "                    },\n",
        "]\n",
        "\n",
        "res, token_dict = get_completion_with_token_count(messages, temp=1)\n",
        "print(f'Response: {res}')\n",
        "print(f'\\n\\nToken Count Dictionary: {token_dict}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgJgNHj2vNk3",
        "outputId": "aff7632c-67fd-4b81-d81b-8921b6edcdd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: A happy carrot so orange and plump, makes tummies dance and hearts jump!\n",
            "\n",
            "\n",
            "Token Count Dictionary: {'prompt_tokens': 51, 'completion_tokens': 18, 'total_tokens': 69}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Prompting is Revolutionizing AI Application Development!\n",
        "\n",
        "In a traditional supervised ML workflow, a sentiment classifier on restaurant review might take maybe\n",
        "- 1 month to get labeled data!\n",
        "- 3 months to train model on data, choosing appropriate model, tune the model and then evaluating it!\n",
        "- 3 months to find a cloud service, upload model on cloud and then run & call model!\n",
        "\n",
        "whereas with prompting-based ML\n",
        "- minutes/hours (if it requires iterative prompting) to get effective prompt!\n",
        "- hours to get run using API calls and start making calls to the model\n",
        "- minutes/hours to call the model and start making inferences\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UL8FnXKC0kZ7"
      }
    }
  ]
}