{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvO+TafT8EJknidb80hTpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmapratik88/LearningAI/blob/main/DeepLearning.AI/02_Building_Systems_with_OpenAI_API/Building_Systems_with_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Systems with the ChatGPT API\n",
        "A [DeepLearning.AI](https://www.deeplearning.ai/short-courses/) short course taught by Andrew NG and Isa Fulford (OpenAI).\n",
        "\n",
        "Coverage:\n",
        "- Building a system that requires more than a single prompt or a single call to an LLM or a large language model\n",
        "- How do you evaluate the input to ensure it doesn't contain problematic content, such as hateful speech, or isn't providing inaccurate or inappropriate answers?\n",
        "- How do we sequentially process the user input in multiple steps to get to the final output shown to the user?\n",
        "- Split complex tasks into a pipeline of subtasks using multistage prompts."
      ],
      "metadata": {
        "id": "3D-fQqiUhCIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNt-EOk7RC2L",
        "outputId": "72cef3a7-0e43-4b8a-c06c-5776313a2d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the current working directory\n",
        "import os; os.chdir('/content/drive/MyDrive/Learnings/DeepLearning.AI/Building_Systems_with_ChatGPT_API')"
      ],
      "metadata": {
        "id": "6B2xaPphRJCK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages/libraries\n",
        "!pip install -q openai\n",
        "\n",
        "from helperFunc import *"
      ],
      "metadata": {
        "id": "oYowacq2TTzt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Model (LLMs)\n",
        "\n",
        "LLMs are sophisticated computer programs capable of generating and analyzing human-quality text and have revolutionized various aspects of Natural Language Processing (NLP).\n",
        "\n",
        "Read more at [ChatGPT Prompt Engineering & LLMs](https://pratikdsharma.com/chatgpt-prompt-engineering/).\n",
        "\n",
        "### Answering how do they work?\n",
        "- For e.g., text generation, prompt: \"I love eating,\" and LLM generates the rest, which might be \"bagels with cream cheese or out with friends.\"\n",
        "\n",
        "- The main tool to train an LLM is supervised learning (x -> y), which predicts the next word repeatedly. For example, your training sets contain a lot of text data, which says, \"My favorite food is a bagel with cream cheese and lox.\" Then, when asking LLM the above prompt again, it might respond to the next word as \"bagel\".\n",
        "\n",
        "- Two major LLMs:\n",
        "  - base LLMs: predicts next word, based on text training data\n",
        "  - instruction LLMs: Try to follow instructions\n",
        "  - Read more at [ChatGPT Prompt Engineering & LLMs](https://pratikdsharma.com/chatgpt-prompt-engineering/).\n",
        "\n",
        "  **How do we go from base to instruction-tuned LLMs?**\n",
        "  - Train a base LLM on loads of data\n",
        "  - Further, train the model:\n",
        "\t- Fine-tune on examples of where the output follows input instructions.\n",
        "   - Obtain human ratings of the quality of different LLM outputs based on whether it is helpful, honest, or harmless.\n",
        "\t- Tune LLM to increase the probability that it generates more highly rated outputs (using RHLF: Reinforcement Learning from Human Feedback).\n",
        "\t- Base LLM training can take months. For e.g., the process of going from the Base LLM to the Instruction Tuned LLM can be done in maybe days on a much more modest size dataset and much more modest size computational resources."
      ],
      "metadata": {
        "id": "DVAm3SRJbATX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion('What is the capital of France?')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYLqg0RRaQSB",
        "outputId": "3036ba9c-38cf-4cb2-eb50-aee8c53af75b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion('Take the letters in lollipop and reverse them!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_Jy2aKh9CN",
        "outputId": "37af56c8-b85d-42d7-d969-5a57df1e28d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reversed letters of \"lollipop\" are \"pillipol\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importance of Tokens\n",
        "  - LLM will take sequences of characters, like \"Learning new things is fun!\" and group the characters together to form tokens that comprise commonly occurring sequences of characters. Since each is a fairly common word, each token in the earlier example will correspond to one word or word in a space.\n",
        "  - In example such as \"Prompting as powerful developer tool\", it will be actually broken down to three tokens with \"prom\", \"pt\", and \"ing\" because those three are commonly occuring sequences of letters.\n",
        "  - For word lollipop, the tokenizer will break this down into three tokens, \"l\", \"oll\", and \"ipop\"."
      ],
      "metadata": {
        "id": "Mh4uQwnZiRPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion('Take the letters in l-o-l-l-i-p-o-p and reverse them!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8ninnCYiMzW",
        "outputId": "764576ba-ccb2-4b5c-bcc2-fdb11680fae4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-o-p-i-l-l-o-l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, each letter was accompanied by dashes in between the letters, LLM tokenizes each of these characters into an individual toekn, making it easier for it to see the individual letters and print them out in reverse order.\n",
        "\n",
        "For English language input, 1 token is around 4 characters or 3/4 of a word.\n",
        "\n",
        "**Token limits:**\n",
        "- Different models have different limits on the number of token in input (aka context) + output (aka completion)\n",
        "- For e.g. for gpt3.5-turbo is ~4000 tokens (input + output)\n",
        "- For an input context that is much longer than this, it will either throw an exception or generate an error.\n",
        "- The limit for a given API request is the combination of the prompt and the completion. For example, if the prompt is 3,000 tokens, the completion cannot be more than around 1,000 tokens.\n"
      ],
      "metadata": {
        "id": "Btkl1G94l-ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXo43-6nl1hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKfSs_xFpBfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}