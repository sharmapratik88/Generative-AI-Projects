{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 26 08:46:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   35C    P8              15W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21239,
     "status": "ok",
     "timestamp": 1716693477968,
     "user": {
      "displayName": "Pratik Sharma",
      "userId": "14254403169221394396"
     },
     "user_tz": -330
    },
    "id": "ZL06qmJ71V4v"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, AwqConfig, AutoConfig\n",
    "\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.llms import VLLM\n",
    "from vllm import LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging to output to a file with a specific format and date format\n",
    "logging.basicConfig(filename='logger.txt',\n",
    "                    format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%d-%b-%y %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINE CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1716693477969,
     "user": {
      "displayName": "Pratik Sharma",
      "userId": "14254403169221394396"
     },
     "user_tz": -330
    },
    "id": "iO2KFH5r2m3-"
   },
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    '''\n",
    "    Class containing constants used throughout the project.\n",
    "    '''\n",
    "    pdf_dir = 'pdfs/'  # Path to the directory containing PDF files to be processed\n",
    "    chunk_size = 2500  # Size of text chunks (in characters) for splitting the document\n",
    "    chunk_overlap = 100  # Number of characters to overlap between adjacent text chunks\n",
    "    return_k = 3  # Number of top results to return from vector similarity searches\n",
    "    embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'  # Name of the pre-trained sentence embedding model\n",
    "    model_name = 'mistralai/Mistral-7B-Instruct-v0.2'  # Name of the base model for quantization\n",
    "    model_save_path = './models'  # Path to save the base model\n",
    "    quant_model_name = 'Mistral-7B-Instruct-v0.2-AWQ'  # Name of the AWQ quantized model\n",
    "    quant_config = {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}  # Configuration for AWQ quantization\n",
    "    quant_save_path = f'./quantized/{quant_model_name}'  # Path to save the AWQ quantized model\n",
    "    username = 'sharmapratik88'  # HuggingFace username\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Device to use for computations (GPU if available, else CPU)\n",
    "    HF_TOKEN = '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'  # HuggingFace token (replace with your own token or a secure way of reading it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA FROM PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1716693477969,
     "user": {
      "displayName": "Pratik Sharma",
      "userId": "14254403169221394396"
     },
     "user_tz": -330
    },
    "id": "w2G5vOBG2qWO"
   },
   "outputs": [],
   "source": [
    "def load_pdf_data():\n",
    "    '''\n",
    "    Loads and processes PDF documents from a specified directory. Each PDF is split into smaller chunks\n",
    "    using a text splitter. The function logs various stages of this process.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of document chunks obtained after splitting all the loaded PDF documents.\n",
    "    '''\n",
    "    logging.info('---------------------------')\n",
    "    logging.info('Data Ingestion')\n",
    "\n",
    "    # Create a list of PDF loaders for each PDF file in the specified directory\n",
    "    loaders = [PyPDFLoader(os.path.join(Constants.pdf_dir, fn)) for fn in os.listdir(Constants.pdf_dir)]\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    # Iterate through each PDF loader\n",
    "    for loader in loaders:\n",
    "        logging.info('Loading raw document: ' + loader.file_path.replace('../', ''))\n",
    "\n",
    "        # Load the raw documents from the PDF file\n",
    "        raw_documents = loader.load()\n",
    "\n",
    "        logging.info('Splitting text .....')\n",
    "\n",
    "        # Initialize the text splitter with the specified chunk size and overlap\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Constants.chunk_size,\n",
    "                                                       chunk_overlap=Constants.chunk_overlap)\n",
    "\n",
    "        # Split the raw documents into chunks\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "        logging.info(f'Length of the PDF after chunking: {str(len(documents))}')\n",
    "\n",
    "        # Add the chunks to the list of all documents\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    logging.info(f'Length of all documents after chunking: {str(len(all_documents))}')\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1716693479092,
     "user": {
      "displayName": "Pratik Sharma",
      "userId": "14254403169221394396"
     },
     "user_tz": -330
    },
    "id": "zRjOLJBS355A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous', metadata={'source': 'pdfs/Llama3 paper.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PDF documents\n",
    "documents = load_pdf_data()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VECTOR DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_vector(docs):\n",
    "    '''\n",
    "    Embeds a list of documents using the SentenceTransformer model and stores the embeddings\n",
    "    in a Chroma vector database. Sets up the Chroma DB to act as a retriever for similarity search.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        retriever: A Chroma retriever object configured to search through the embedded documents.\n",
    "    '''\n",
    "    # Initialize the embeddings using the specified model\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=Constants.embedding_model)\n",
    "\n",
    "    # Initialize the vector DB (Chroma)\n",
    "    logging.info('Initiating the Chroma DB vectorization step')\n",
    "\n",
    "    # Create a Chroma vector DB from the documents\n",
    "    vectordb = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "    # Set up the Chroma DB as a retriever with a specific number of results to return (k)\n",
    "    retrieve = vectordb.as_retriever(search_kwargs={'k': Constants.return_k})\n",
    "\n",
    "    return retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = embed_vector(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWQ QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1716693477970,
     "user": {
      "displayName": "Pratik Sharma",
      "userId": "14254403169221394396"
     },
     "user_tz": -330
    },
    "id": "pFr89VOr25L6"
   },
   "outputs": [],
   "source": [
    "def quantize(docs):\n",
    "    \"\"\"\n",
    "    Quantizes a specified language model from HuggingFace and saves it locally.\n",
    "    The function also uploads the quantized model to the HuggingFace Hub.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of document objects containing text data to be used for calibration during quantization.\n",
    "\n",
    "    Steps:\n",
    "        1. Load environment variables from a .env file.\n",
    "        2. Log in to HuggingFace using the token from environment variables.\n",
    "        3. Create necessary directories if they don't exist.\n",
    "        4. Load the model and tokenizer from HuggingFace.\n",
    "        5. Quantize the model using the provided calibration data.\n",
    "        6. Save the quantized model and tokenizer locally.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load environment variables from a .env file\n",
    "    _ = load_dotenv()\n",
    "\n",
    "    # HuggingFace login using the token from environment variables\n",
    "    login(token=os.environ.get('HF_TOKEN'))\n",
    "\n",
    "    # Create paths if they don't exist\n",
    "    if not os.path.exists(Constants.quant_save_path):\n",
    "        os.makedirs(Constants.quant_save_path)\n",
    "\n",
    "    if not os.path.exists(Constants.model_save_path):\n",
    "        os.makedirs(Constants.model_save_path)\n",
    "\n",
    "    # Log the initialization of the specified model from HuggingFace\n",
    "    logging.info(f'Loading {Constants.model_name} model from HuggingFace.')\n",
    "\n",
    "    # Load the model from HuggingFace with specific configurations\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        Constants.model_name,\n",
    "        **{'low_cpu_mem_usage': True, 'use_cache': False},  # Use low CPU memory and disable cache\n",
    "        cache_dir=Constants.model_save_path  # Specify the cache directory\n",
    "    )\n",
    "\n",
    "    # Load the tokenizer from HuggingFace with specific configurations\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        Constants.model_name,\n",
    "        trust_remote_code=True,  # Trust remote code for loading the tokenizer\n",
    "        use_fast=True,  # Use the fast tokenizer implementation\n",
    "        cache_dir=Constants.model_save_path  # Specify the cache directory\n",
    "    )\n",
    "\n",
    "    # Quantize the model using the specified configuration and calibration data\n",
    "    model.quantize(tokenizer, quant_config=Constants.quant_config,\n",
    "                   calib_data=[page.page_content for page in docs])\n",
    "\n",
    "    # Save the quantized model and tokenizer to the specified path\n",
    "    model.save_quantized(Constants.quant_save_path)\n",
    "    tokenizer.save_pretrained(Constants.quant_save_path)\n",
    "    logging.info('Quantized model and tokenizers saved to the path specified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "83da44200a1f4e7cb671b04c6fd39f4e",
      "2f5631aff2a6402a966ae94cff5cb246",
      "004596aee6574b49b18a7d66ebf42b29",
      "93005b8d47c64c4c92f12db09a39663f",
      "619eec0d302445338f63f3b06c79c1f3",
      "8f879a884ece4dcd93a77a5a792e3f59",
      "6abf93e022814973941133d25e8eeda3",
      "e6e86df3507d435993db2c34cb32b736",
      "bb622ed94b7544ee8b4c2c708a3bd453",
      "f192382bcd134f48a86a035d51562002",
      "4d89866d9d3c483293f47b981aeb61ea",
      "1912da8da8904415b966b5d2dd4e09d6",
      "d8ed960a3a004a0ca077e009cfa8f336",
      "3d18de126b4b47c8a25ee11e58171c95",
      "728bf089842a4873bbed5f99164cfce0",
      "580b0b7bbdd34d15b4f158bb1a56f913",
      "567a265d937a47bf95ce6c46e9d45cf6",
      "af193ef1b9fe48878ec064799caac4fb",
      "940361457a5143d4845160c408cfa671",
      "a6adf3bc7b0c400a8c765c5389a3fa6f",
      "6617d5638fcc4bfcbd79edb9713000f1",
      "b30dc7d267d74a638c534cec18f2dfd7"
     ]
    },
    "id": "9Su1PnrJ4DZL",
    "outputId": "2e904203-b363-4fb0-982a-e852ac9f0ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae9d0c775ef41d59af2f6e8b4a8a40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b5109416b44e70b9aa0cf8edbd4458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 32/32 [09:43<00:00, 18.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Perform the quantization process (assuming this modifies the model in place or sets up necessary files)\n",
    "_ = quantize(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9924470af6a247b6ac10721e620e2d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f164bdf9fd4fa69ee2552ae522c778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75a904908874edcb968a29f1e0cc16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sharmapratik88/Mistral-7B-Instruct-v0.2-AWQ/commit/3b4e2ee94a959710b2502f104a71922038121e06', commit_message='Upload folder using huggingface_hub', commit_description='', oid='3b4e2ee94a959710b2502f104a71922038121e06', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the HuggingFace API with the token\n",
    "api = HfApi(token=os.environ.get('HF_TOKEN'))\n",
    "\n",
    "# Create a new repository on HuggingFace for the quantized model\n",
    "api.create_repo(repo_id=f'{Constants.username}/{Constants.quant_model_name}', repo_type='model')\n",
    "\n",
    "# Upload the quantized model folder to the HuggingFace repository\n",
    "api.upload_folder(repo_id=f'{Constants.username}/{Constants.quant_model_name}',\n",
    "                  folder_path=Constants.quant_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD THE QUANTIZED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 08:58:38 config.py:205] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-26 08:58:38 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='sharmapratik88/Mistral-7B-Instruct-v0.2-AWQ', speculative_config=None, tokenizer='sharmapratik88/Mistral-7B-Instruct-v0.2-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=sharmapratik88/Mistral-7B-Instruct-v0.2-AWQ)\n",
      "INFO 05-26 08:58:38 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-26 08:58:39 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-26 08:58:39 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 08:58:41 model_runner.py:175] Loading model weights took 3.8811 GB\n",
      "INFO 05-26 08:58:44 gpu_executor.py:114] # GPU blocks: 3367, # CPU blocks: 2048\n",
      "INFO 05-26 08:58:45 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-26 08:58:45 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-26 08:58:52 model_runner.py:1017] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "# Set the environment variable 'CUDA_LAUNCH_BLOCKING' to '1' to help with debugging CUDA errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Construct the model ID using the quantized model name from the Constants module\n",
    "model_id = f'sharmapratik88/{Constants.quant_model_name}'\n",
    "\n",
    "# Initialize the vLLM model with specific parameters\n",
    "llm_q = VLLM(\n",
    "    model=model_id,  # The model ID to load from Hugging Face\n",
    "    trust_remote_code=True,  # Allow execution of remote code for custom model loading\n",
    "    max_new_tokens=4096,  # Maximum number of new tokens to generate\n",
    "    temperature=0.9,  # Sampling temperature for text generation\n",
    "    vllm_kwargs={\n",
    "        'quantization': 'awq',  # Use AWQ quantization for the model\n",
    "        'gpu_memory_utilization': 0.6  # Set GPU memory utilization to 60%\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RETRIEVE RESPONSE FROM THE PDF(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHybrid Search is a search methodology used for Retrieval-as-a-Service (RAG) systems that combines multiple search techniques to improve overall accuracy. In this study, the authors explored keyword-based similarity search, dense vector-based, and semantic-based sparse encoder-based search, and integrated these techniques to formulate hybrid queries. The goal is to elevate search capabilities and capture nuanced relationships between terms, thereby providing a more authentic representation of user intent and document relevance. The authors used the Sparse Encoder Model-based index with sparse encoder query + match query and combinations of multi match queries for their experiments.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RetrievalQA chain using the vLLM model and the retriever\n",
    "qa_with_sources = RetrievalQA.from_chain_type(llm=llm_q, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "# Define the query to be asked to the LLM\n",
    "query = 'What is Hybrid Search?'  # The question to be answered by the LLM\n",
    "logging.info(f'Question to the LLM: {query}')  # Log the question being asked\n",
    "\n",
    "# Get the response from the retrieval QA chain\n",
    "response = qa_with_sources(query)['result']\n",
    "\n",
    "# Log the answer received from the LLM\n",
    "logging.info(f'Answer from the LLM: {response}')\n",
    "\n",
    "# Return the response\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMcrpLfv3iI90HDh0p74YMX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "004596aee6574b49b18a7d66ebf42b29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6e86df3507d435993db2c34cb32b736",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb622ed94b7544ee8b4c2c708a3bd453",
      "value": 12
     }
    },
    "1912da8da8904415b966b5d2dd4e09d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8ed960a3a004a0ca077e009cfa8f336",
       "IPY_MODEL_3d18de126b4b47c8a25ee11e58171c95",
       "IPY_MODEL_728bf089842a4873bbed5f99164cfce0"
      ],
      "layout": "IPY_MODEL_580b0b7bbdd34d15b4f158bb1a56f913"
     }
    },
    "2f5631aff2a6402a966ae94cff5cb246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f879a884ece4dcd93a77a5a792e3f59",
      "placeholder": "​",
      "style": "IPY_MODEL_6abf93e022814973941133d25e8eeda3",
      "value": "Fetching 12 files: 100%"
     }
    },
    "3d18de126b4b47c8a25ee11e58171c95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_940361457a5143d4845160c408cfa671",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6adf3bc7b0c400a8c765c5389a3fa6f",
      "value": 0
     }
    },
    "4d89866d9d3c483293f47b981aeb61ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "567a265d937a47bf95ce6c46e9d45cf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "580b0b7bbdd34d15b4f158bb1a56f913": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "619eec0d302445338f63f3b06c79c1f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6617d5638fcc4bfcbd79edb9713000f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6abf93e022814973941133d25e8eeda3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "728bf089842a4873bbed5f99164cfce0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6617d5638fcc4bfcbd79edb9713000f1",
      "placeholder": "​",
      "style": "IPY_MODEL_b30dc7d267d74a638c534cec18f2dfd7",
      "value": " 0/3 [00:00&lt;?, ?it/s]"
     }
    },
    "83da44200a1f4e7cb671b04c6fd39f4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f5631aff2a6402a966ae94cff5cb246",
       "IPY_MODEL_004596aee6574b49b18a7d66ebf42b29",
       "IPY_MODEL_93005b8d47c64c4c92f12db09a39663f"
      ],
      "layout": "IPY_MODEL_619eec0d302445338f63f3b06c79c1f3"
     }
    },
    "8f879a884ece4dcd93a77a5a792e3f59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93005b8d47c64c4c92f12db09a39663f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f192382bcd134f48a86a035d51562002",
      "placeholder": "​",
      "style": "IPY_MODEL_4d89866d9d3c483293f47b981aeb61ea",
      "value": " 12/12 [00:00&lt;00:00, 463.52it/s]"
     }
    },
    "940361457a5143d4845160c408cfa671": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6adf3bc7b0c400a8c765c5389a3fa6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af193ef1b9fe48878ec064799caac4fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b30dc7d267d74a638c534cec18f2dfd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb622ed94b7544ee8b4c2c708a3bd453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8ed960a3a004a0ca077e009cfa8f336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_567a265d937a47bf95ce6c46e9d45cf6",
      "placeholder": "​",
      "style": "IPY_MODEL_af193ef1b9fe48878ec064799caac4fb",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "e6e86df3507d435993db2c34cb32b736": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f192382bcd134f48a86a035d51562002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
